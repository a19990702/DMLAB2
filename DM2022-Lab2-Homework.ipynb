{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 許昱呈\n",
    "\n",
    "Student ID: 111164519\n",
    "\n",
    "GitHub ID: a19990702\n",
    "\n",
    "Kaggle name: BrianHSU8872\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "[Snapshot](img/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM2022-Lab2-master Repo](https://github.com/keziatamus/DM2022-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm2022-isa5810-lab2-homework) regarding Emotion Recognition on Twitter by this link https://www.kaggle.com/t/2b0d14a829f340bc88d2660dc602d4bd. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (60-x)/6 + 20 points, where x is your ranking in the leaderboard (ie. If you rank 3rd your score will be (60-3)/6 + 20 = 29.5% out of 30%)   \n",
    "    Submit your last submission __BEFORE the deadline (Nov. 22th 11:59 pm, Tuesday)_. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 25th 11:59 pm, Friday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python 3.6.10 :: Anaconda, Inc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim: 4.2.0\n",
      "tensorflow: 2.6.2\n",
      "keras: 2.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import gensim\n",
    "import tensorflow\n",
    "import keras\n",
    "%matplotlib inline\n",
    "print(\"gensim: \" + gensim.__version__)\n",
    "print(\"tensorflow: \" + tensorflow.__version__)\n",
    "print(\"keras: \" + keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "### prepare the data\n",
    "data_identification = pd.read_csv(\"dm2022-isa5810-lab2-homework/data_identification.csv\")\n",
    "emotion = pd.read_csv(\"dm2022-isa5810-lab2-homework/emotion.csv\")\n",
    "sampleSubmission = pd.read_csv(\"dm2022-isa5810-lab2-homework/sampleSubmission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "file = open(\"dm2022-isa5810-lab2-homework/tweets_DM.json\",'r',encoding='utf-8')\n",
    "datas =[]\n",
    "for line in file.readlines():\n",
    "    data = json.loads(line)\n",
    "    data['hashtags'] =  data['_source']['tweet']['hashtags']\n",
    "    data['tweet_id'] = data['_source']['tweet']['tweet_id']\n",
    "    data['text'] = data['_source']['tweet']['text']\n",
    "    del data['_source']\n",
    "    datas.append(data)\n",
    "df = pd.DataFrame(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df_data_identification = df.merge(data_identification, on = 'tweet_id', how = 'left')\n",
    "# splite the data to two piles, data_x for training module, and put test_x data into module and get answer \n",
    "data_x = df_data_identification.loc[df_data_identification['identification'] == 'train']\n",
    "test_x = df_data_identification.loc[df_data_identification['identification'] == 'test']\n",
    "data_answer_y = data_x.merge(emotion, on = 'tweet_id', how = 'left' )['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n## check library\\nimport gensim\\nimport nltk \\n## ignore warnings\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\n# # if you want to see the training messages, you can use it\\n# import logging\\n# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\\n\\n## the input type\\ndata_x['text_tokenized'] = data_x['text'].apply(lambda x: nltk.word_tokenize(x))\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## check library\n",
    "import gensim\n",
    "import nltk \n",
    "## ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# # if you want to see the training messages, you can use it\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "## the input type\n",
    "data_x['text_tokenized'] = data_x['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# build analyzers (bag-of-words)\n",
    "# I try to use TFIDF into analyzers, and kill the stop words\n",
    "BOW_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "# 1. Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "BOW_vectorizer.fit(data_x['text'])\n",
    "\n",
    "# 2. Transform documents to document-term matrix.\n",
    "train_data_BOW_features = BOW_vectorizer.transform(data_x['text'])\n",
    "test_data_BOW_features = BOW_vectorizer.transform(test_x['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "feature_names = BOW_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ferry\\miniconda3\\envs\\py36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1455563, 500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# build analyzers (bag-of-words)\n",
    "#use TFIDF analyzers and decrease the feature to the 500\n",
    "BOW_500 = TfidfVectorizer(max_features=500, tokenizer=nltk.word_tokenize,stop_words='english') \n",
    "\n",
    "# apply analyzer to training data\n",
    "BOW_500.fit(data_x['text'])\n",
    "\n",
    "train_data_BOW_features_500 = BOW_500.transform(data_x['text'])\n",
    "\n",
    "## check dimension\n",
    "train_data_BOW_features_500.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#using decision tree to train the module\n",
    "X_train = BOW_500.transform(data_x['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# y_train is the x_train datasets answer\n",
    "y_train = data_answer_y\n",
    "\n",
    "X_test = BOW_500.transform(test_x['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "## build DecisionTree model\n",
    "DT_model = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "## training!\n",
    "DT_model = DT_model.fit(X_train, y_train)\n",
    "\n",
    "## predict!\n",
    "y_train_pred = DT_model.predict(X_train)\n",
    "y_test_pred = DT_model.predict(X_test)\n",
    "\n",
    "## so we get the pred result\n",
    "sampleSubmission['emotion'] = y_test_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                trust\n",
       "1                trust\n",
       "2              disgust\n",
       "3         anticipation\n",
       "4         anticipation\n",
       "              ...     \n",
       "411967    anticipation\n",
       "411968    anticipation\n",
       "411969    anticipation\n",
       "411970         sadness\n",
       "411971         sadness\n",
       "Name: emotion, Length: 411972, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleSubmission['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sampleSubmission.to_csv(\"dm2022-isa5810-lab2-homework/sampleSubmission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third part\n",
    "In the homework, \n",
    "I use the way in the lab2_master to use the BOW and train the module and get answer, \n",
    "I can get 18.55% accuracy\n",
    "afterthat,\n",
    "I use TFIDF to be the analyzer, get 20% accuracy\n",
    "All of the training module use the decision Tree\n",
    "\n",
    "Because of a lots of homework in November. I don't use a lots of time to find the way to get better answer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
